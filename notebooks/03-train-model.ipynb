{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46a760f-c662-45fe-a0e6-711d8fc7a499",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "534de7b4-17d6-40df-96b9-02ed625edcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df32f68f-7c12-43e2-a068-d177f12832b6",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e698748b-20de-4063-ab36-a5fff8d24dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d1deb-14b9-4179-81d0-0aef76b6bb84",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570dcebd-ffa2-4660-8dc8-922c858d1db5",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8a7df-c7bf-4ab7-80c6-37da7d597ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombatForecastDataset(Dataset):\n",
    "    def __init__(self, path_lb_our, path_lb_bandit, path_fc_action, path_fc_our, path_fc_bandit, sos_value=9999.0):\n",
    "        self.lb_our = np.load(path_lb_our)          # (N, 10, 18)\n",
    "        self.lb_bandit = np.load(path_lb_bandit)    # (N, 10, 13)\n",
    "        self.fc_action = np.load(path_fc_action)    # (N, 3, 10)\n",
    "        self.fc_our = np.load(path_fc_our)          # (N, 3, 18)\n",
    "        self.fc_bandit = np.load(path_fc_bandit)    # (N, 3, 13)\n",
    "        self.sos_value = sos_value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lb_our)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Encoder input: concat states\n",
    "        src = np.concatenate([self.lb_our[idx], self.lb_bandit[idx]], axis=-1)  # (10, 31)\n",
    "\n",
    "        # Decoder input: action forecast\n",
    "        tgt = self.fc_action[idx]  # (3, 10)\n",
    "        sos = np.ones((1, 10)) * self.sos_value\n",
    "        tgt_input = np.vstack([sos, tgt[:-1]])  # (3, 10)\n",
    "\n",
    "        # Decoder output target: concat future states\n",
    "        tgt_output = np.concatenate([self.fc_our[idx], self.fc_bandit[idx]], axis=-1)  # (3, 31)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(src, dtype=torch.float32),        # encoder input (10, 31)\n",
    "            torch.tensor(tgt_input, dtype=torch.float32),  # decoder input (3, 10)\n",
    "            torch.tensor(tgt_output, dtype=torch.float32)  # decoder target (3, 31)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c63c89-2a3e-4589-9b0c-b5cd32394eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            src_vocab_size,\n",
    "            tgt_vocab_size,\n",
    "            d_model=256,\n",
    "            nhead=4,\n",
    "            num_encoder_layers=3,\n",
    "            num_decoder_layers=3,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            src,\n",
    "            tgt,\n",
    "            src_mask=None,\n",
    "            tgt_mask=None,\n",
    "            src_key_padding_mask=None,\n",
    "            tgt_key_padding_mask=None,\n",
    "            memory_key_padding_mask=None):\n",
    "        \n",
    "        src = self.positional_encoding(self.src_embedding(src) * math.sqrt(self.d_model)).transpose(0, 1)\n",
    "        tgt = self.positional_encoding(self.tgt_embedding(tgt) * math.sqrt(self.d_model)).transpose(0, 1)\n",
    "\n",
    "        output = self.transformer(\n",
    "            src, tgt, src_mask=src_mask, tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "\n",
    "        return self.fc_out(output.transpose(0, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
